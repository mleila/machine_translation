{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Torchtext Detailed Tutorial](https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "\n",
    "from translator.languages import Language\n",
    "from translator.data import EuroParl, make_dataset, make_fields\n",
    "from translator.networks import Encoder, Decoder, Encode_Decoder_Model\n",
    "from translator.utils import Directory, tokenize_sent, translate_sentence, load_checkpoint, save_checkpoint\n",
    "\n",
    "SOURCE_DIR = '..'\n",
    "directory = Directory(SOURCE_DIR)\n",
    "\n",
    "ENGLISH, FRENCH = 'en', 'fr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Build Languages Vocab and Save it locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10004, 10004)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english = Language('english')\n",
    "english.load_language(directory.languages_path / 'en.p')\n",
    "\n",
    "french = Language('french')\n",
    "french.load_language(directory.languages_path / 'fr.p')\n",
    "\n",
    "len(english.vocab), len(french.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Build Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling 50000 out of 2007724 samples\n"
     ]
    }
   ],
   "source": [
    "## create tabular dataset (run once)\n",
    "# constants\n",
    "EURO_PARL_DATA_PATH = directory.data_path / 'fr-en'\n",
    "\n",
    "# dataset abstraction\n",
    "euro_parl = EuroParl(\n",
    "    data_dir=EURO_PARL_DATA_PATH, \n",
    "    lang1_name=ENGLISH, \n",
    "    lang2_name=FRENCH, \n",
    "    sample_size=50_000)\n",
    "euro_parl.train_valid_test_split(valid_size=0.3, test_size=0.2)\n",
    "euro_parl.to_csv(EURO_PARL_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build torch datasets\n",
    "english_data_field = (ENGLISH, english.field)\n",
    "french_data_field = (FRENCH, french.field)\n",
    "train, val, test = make_dataset(\n",
    "    english_data_field,\n",
    "    french_data_field,\n",
    "    directory.data_path / 'fr-en/train.csv',\n",
    "    directory.data_path / 'fr-en/valid.csv',\n",
    "    directory.data_path / 'fr-en/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# model parameters\n",
    "load_model = False\n",
    "\n",
    "sequenec_size_encoder = len(english.vocab)\n",
    "sequenec_size_decoder = len(french.vocab)\n",
    "output_size = len(french.vocab)\n",
    "\n",
    "embedding_dim = 20\n",
    "hidden_size = 5\n",
    "num_layers = 1\n",
    "\n",
    "encoder = Encoder(sequenec_size_encoder, embedding_dim, hidden_size, num_layers).to(device)\n",
    "decoder = Decoder(sequenec_size_decoder, embedding_dim, output_size, hidden_size, num_layers).to(device)\n",
    "model = Encode_Decoder_Model(encoder, decoder, device).to(device)\n",
    "\n",
    "if load_model:\n",
    "    checkpoint_path = '../data/models/checkpoint_epoch=4.pth.tar'\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    load_checkpoint(checkpoint, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntest_sentence = \"The world is watching today France is issuing an order.\"\\ntokenized_sent = tokenize_sent(test_sentence, ENGLISH, english_field, device)\\n\\ntokenized_sent.shape\\n\\nh_0, c_0 = encoder(tokenized_sent)\\nstart_token = torch.tensor([english_field.vocab.stoi[\\'hello\\']]) # one word, one batch\\noutput, hidden, cell = decoder(start_token, h_0, c_0)\\nvocab_probabilities = output.squeeze(0)\\nbest_guess = torch.argmax(vocab_probabilities)\\nfrench_field.vocab.itos[best_guess]\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# task: translate from English to French\n",
    "'''\n",
    "test_sentence = \"The world is watching today France is issuing an order.\"\n",
    "tokenized_sent = tokenize_sent(test_sentence, ENGLISH, english_field, device)\n",
    "\n",
    "tokenized_sent.shape\n",
    "\n",
    "h_0, c_0 = encoder(tokenized_sent)\n",
    "start_token = torch.tensor([english_field.vocab.stoi['hello']]) # one word, one batch\n",
    "output, hidden, cell = decoder(start_token, h_0, c_0)\n",
    "vocab_probabilities = output.squeeze(0)\n",
    "best_guess = torch.argmax(vocab_probabilities)\n",
    "french_field.vocab.itos[best_guess]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 / 1]\n",
      "Translated example sentence: \n",
      " limiter dans graves on on on on on ans avons dans publics on on ans avons dans publics on on ans avons dans publics on on ans avons dans publics on on ans avons dans publics on on ans avons dans publics on on ans avons dans publics on on\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-e0b369d960fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Back prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Clip to avoid exploding gradient issues, makes sure grads are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pDL/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training params\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "# making iterators\n",
    "train_iterator = torchtext.data.BucketIterator(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "pad_idx = english_field.vocab.stoi[\"<pad>\"]\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "test_sentence = \"The world is watching today France is issuing an order.\"\n",
    "tokenized_sent = tokenize_sent(test_sentence, ENGLISH, english, device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "    \n",
    "    # setup model for evaluation\n",
    "    model.eval()\n",
    "    translated_sentence = translate_sentence(model, tokenized_sent, FRENCH, french, device)\n",
    "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "\n",
    "        # Get input and targets and get to cuda\n",
    "        inp_data = batch.en.to(device)\n",
    "        target = batch.fr.to(device)\n",
    "\n",
    "        # Forward prop\n",
    "        output = model(inp_data, target, len(french.vocab))\n",
    "\n",
    "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
    "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
    "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
    "        # way that we have output_words * batch_size that we want to send in into\n",
    "        # our cost function, so we need to do some reshapin. While we're at it\n",
    "        # Let's also remove the start token while we're at it\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "../data/models/encoder_decoder_1/1609185187.pth.tar\n",
      "=> Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "load_model = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# model parameters\n",
    "model_name = 'encoder_decoder_1'\n",
    "\n",
    "sequenec_size_encoder = len(english.vocab)\n",
    "sequenec_size_decoder = len(french.vocab)\n",
    "output_size = len(french.vocab)\n",
    "\n",
    "embedding_dim = 200\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "\n",
    "encoder = Encoder(sequenec_size_encoder, embedding_dim, hidden_size, num_layers).to(device)\n",
    "decoder = Decoder(sequenec_size_decoder, embedding_dim, output_size, hidden_size, num_layers).to(device)\n",
    "model = Encode_Decoder_Model(encoder, decoder, device).to(device)\n",
    "\n",
    "if load_model:\n",
    "    checkpoint_path = directory.get_latest_checkpint(model_name)\n",
    "    print(checkpoint_path)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device(device))\n",
    "    load_checkpoint(checkpoint, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m. <unk> , , , , pas <unk> <unk> <eos>'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"The world is watching today France is issuing an order.\"\n",
    "tokenized_sent = tokenize_sent(test_sentence, ENGLISH, english_field, device)\n",
    "\n",
    "translate_sentence(model, tokenized_sent, dest_language=FRENCH, language_field=french.field, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0],\n",
       "        [   2],\n",
       "        [ 193],\n",
       "        [  10],\n",
       "        [   0],\n",
       "        [ 201],\n",
       "        [   0],\n",
       "        [  10],\n",
       "        [5045],\n",
       "        [  32],\n",
       "        [ 176],\n",
       "        [   4],\n",
       "        [   0]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
